{
  "Docs/BoundsControl.html": {
    "href": "Docs/BoundsControl.html",
    "title": "Bounds Control | UXT Documentation",
    "keywords": "BoundsControl Bounds Control is a component that allows the user to change the position, rotation, and size of an actor, using affordances . Affordances are grabbable areas on corners, edges, and faces of the actor's bounding box. To enable bounds control on an actor, add a UxtBoundsControl component. The component has a default configuration that can be tweaked to change the behavior and appearance as needed. Bounds Control Presets These presets configure the overall set of affordances that are created and the way they affect the actor transform: Default : Uniform resizing with corners and rotation with edges. Slate2D : Only front corners and edges are shown, all resize. AllResize : Full set of affordances, all resizing. AllTranslate : Full set of affordances, all translating. AllScale : Full set of affordances, all scaling. AllRotate : Full set of affordances, all rotating. Affordance Configuration There are four types of affordances: Corner : Each corner of the bounding box. Can move in all three dimensions. Edge : Middle of each edge of the bounding box. Movement is restricted to the plane perpendicular to the edge. Face : Center of each side of the bounding box. Movement is restricted to the axis along the face normal. Center : Center of the bounding box (not enabled by default). Grabbing and moving an affordance can be configured for different effects on the actor transform: Resize : Move only one side of the bounding box. Translate : Move both sides of the bounding box in parallel. Scale : Scale the bounding box, moving both sides in opposite directions. Rotate : Rotate the bounding box about its center point. Affordance classes To modify the appearance of the affordances the user can replace the default classes used by the bounds control. A different class is used for each of the four kinds of affordance (Corner, Edge, Face, Center). These classes are instantiated at runtime by the bounds control component to create grabbable actors. When creating custom affordance blueprints a few conditions should be kept in mind: The affordance should by default be oriented in the forward , right , up direction. Each affordance instance is rotated by the bounds control to match its placement on the bounding box. The affordance should have a UxtGrabTargetComponent to make it grabbable. The bounds control will use these components to react to user input. The affordance does not need to handle user input or grab events itself. It is automatically placed by the bounds control. For customizing affordances it is recommended to use the BoundsControl/BP_DefaultAffordanceBase blueprint as a base class or copy it."
  },
  "Docs/CONTRIBUTING.html": {
    "href": "Docs/CONTRIBUTING.html",
    "title": "Contributing | UXT Documentation",
    "keywords": "Contributing to UX Tools for Unreal Engine Due to the early stage of the project and the likelihood of internal refactors, we are not in a position to accept external contributions via pull requests at this time. However, contributions and feedback in the shape of bug reports, suggestions and feature requests are welcome and encouraged. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com . This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments."
  },
  "Docs/FarBeam.html": {
    "href": "Docs/FarBeam.html",
    "title": "Far Beam | UXT Documentation",
    "keywords": "Far Beam The far beam component allows the user to visualise elements in the scene they can interact with from afar. If you wish to customise the look of the beam, you can set the beam material by calling SetBeamMaterial with the material you wish to use. This material can have the following parameters if you wish to use them in your material. IsGrabbing : Scalar parameter. This value will be 0.0f if the user is not grabbing with the far cursor, 1.0f if they are. handIndex : Scalar parameter. This value will be 0.0f for the left hand, 1.0f for the right hand. The default material for the far beam has the following parameters: Far Beam Material Interface Base Color Color value for the solid beam Emissive Emissive intensity for the solid beam Gradation Hardness Scalar value to control the gradation hardness Gradation Multiply Scalar value for the gradation of the solid beam Gradation Offset Scalar value for the gradation start position offset Gradation Scale Scalar value to control the gradation scale Dotted Base Color Color value for the dotted beam LineNumber Scalar value to control the dot amount in the beam MiddleFade Scalar value to control the fade-out intensity of the dotted beam MidFadeOffset Scalar value to control the fade-out position of the dotted beam IsGrabbing System value for if the user is currently grabbing handIndex System value used for distinguishing left from right hand"
  },
  "Docs/FollowComponent.html": {
    "href": "Docs/FollowComponent.html",
    "title": "Follow Component | UXT Documentation",
    "keywords": "FollowComponent The FollowComponent is used to keep objects \"following\" the user by applying a set of constraints on the component's owner. Usage Place a FollowComponent on your actor. In this example, we created an empty actor and added some geometry and text to make a panel that we want to keep in front of the user. Since we also want it to always face the user, we've set the \"Orientation Type\" to \"FaceCamera\". Hint: The camera looks down the +X axis, so you may want the front face of your content be in the -X direction. Behavior Details The FollowComponent has three different constraints that keeps its owner in front of the camera: Angular Clamp, Distance Clamp, and Orientation. The combination of Angular and Distance Clamp creates a frustum in front of the camera where its owner can be. If its owner is outside that frustum it is adjusted. Angular Clamp : The objective of this constraint is to ensure that the reference forward vector remains within the bounds set by the leashing parameters. To do this, determine the angles between toTarget and the leashing bounds about the global Z-axis and the reference's Y-axis. If the toTarget falls within the leashing bounds, then we don't have to modify it. Otherwise, we apply a correction rotation to bring it within bounds. This will ensure that the its owner stays within the top, bottom, right and left planes of the frustum. Distance Clamp : The objective of this constraint is to ensure that the following actor stays within bounds set by the distance parameters. To do this, we measure the current distance from the camera to the its owner. If the distance is within the MinimumDistance and MaximumDistance then we don't have to modify it. Otherwise, we push away or pull in the its owner along the reference forward vector. This will ensure that the its owner stays within the near and far planes of the frustum. Orientation : The two options provided are constant FaceCamera or WorldLock. While world locked there are three conditions that will cause the its owner to face the camera: Angular Clamps Distance Clamps The angle between the forward vector of the its owner and toTarget vector (vector between the camera and the its owner) is larger than dead zone angle parameter"
  },
  "Docs/Graphics.html": {
    "href": "Docs/Graphics.html",
    "title": "Graphics | UXT Documentation",
    "keywords": "Graphics UX Tools contains a handful of graphics and rendering techniques to implement Fluent Design System principles, and remain performant on Mixed Reality devices. Materials Many mobile stereo headsets are fill rate bound, to reduce fill rate materials should be as simple, or inexpensive as possible. A handful of inexpensive material instances are included with the toolkit which derive from M_SimpleLit_Color. M_SimpleLit_Color uses the Unreal unlit lighting model to avoid extra computations, but uses the MF_SimpleLit material function to perform basic realtime Blinn-Phong lighting on a single (virtual) directional light. The virtual directional light's properties are passed in via the MPC_UXSettings material parameter collection . Note Unreal's built in light types are not included within MF_SimpleLit calculations. MPC_UXSettings contains global shader constants that are used to drive lighting effects as well as UI effects. For example, the left and right pointer positions are updated each frame within MPC_UXSettings to drive lighting effects emitted from the hand interaction pointers. Shaders To achieve visual parity with the HoloLens 2 shell, a couple of shaders exist in the \"UX Tools plugin root\"/Shaders/Public/ directory. A shader source directory mapping is created by the UX Tools plugin to allow any UE4 material to reference shaders within that directory as /Plugin/UXTools/Public/Shader_Name.ush . Note Note, many of the shaders within this directory are generated from an external tool, and are not formatted for user readability. A couple of shaders are described below in more detail, as well as any special considerations which must be made when using the shader. Button Box The M_ButtonBox material and Bounding_Box.ush shader generate glowing edges on a cube with optional circular blob. The blob appears only on the active face set with the Blob_Active_Face parameter. The shader must be used with the correct corresponding cube model. The special model enables the shader to render only the visible areas of the surfaces. The last two letters of the SM_FrontPlate_xx model name indicates which face is active. For example, SM_FrontPlate_PY indicates the positive y-axis. Rounded Edge Thick The M_RoundedEdgeThick material and Rounded_Edge_Thick.ush shader are designed to render a quad as an outlined rounded rectangle with depth. The shader works with a special mesh that approximates the round corners using triangles and includes additional information for anti-aliased silhouette edges. UX Tools includes several models named SM_BackPlateRoundedThick_# where # is the number of triangles per rounded corner. Use the model with the fewest triangles that works for your scenario. Thick Finger Ring The M_FingerTipRing material and Thick_Finger_Ring.ush shader with the M_FingerTipRing model are used to create a volumetric finger tip affordance. Bounding Box The M_BoundsControl material and Bounding_Box.ush shader generate glowing edges on a cube with optional glowing blob and on/off transition effect. The shader must be used with the SM_BoundsControl model. The special models enable the shader to render only the visible areas of the box surfaces. The above shader uses additive blending; if alpha blending is preferred a second method of creating the Bounding Box effects exists which works by splitting the box affordance into parts: The finger tip blob visuals are rendered using the SM_BoundsControlFace model with the M_BoundsControlFace material. Six copies of the SM_BoundsControlEdge model are scaled and offset to create the bound edges and rendered with the M_BoundsControlEdge shader. See also Pressable button component Bounds Control"
  },
  "Docs/HandConstraintComponent.html": {
    "href": "Docs/HandConstraintComponent.html",
    "title": "Hand Constraint Component | UXT Documentation",
    "keywords": "Hand Constraint Component Component that calculates a goal based on hand tracking and moves the owning actor. It keeps the actor position and rotation aligned with a hand while avoiding overlap with fingers. Several zones around the hand supported: radial and ulnar for the thumb side and its opposite, as well as above and below the hand. The goal position is computed by casting a ray in the direction of the one at a bounding box around the hand joints. The constraint can be oriented on either the hand rotation alone or facing the player. Usage Create a HandConstraintComponent on an Actor. Set the Hand property to select which hand should be tracked. If 'Any Hand' is selected, the first tracked hand will be used and switch to the opposite when tracking is lost. The Zone defines the general area around the hand that the actor is placed in. GoalMargin can be used to increase the distance from the hand for larger actors. At runtime the component will move the actor towards the goal position and rotation. Movement can be disabled with the MoveOwningActor option. In that case the goal position and rotation will still be computed and can be used in blueprints. The component will by default use smoothing to avoid jittering artifacts resulting from hand tracking. Smoothing can be disabled by setting LocationLerpTime and/or RotationLerpTime properties to zero. The higher these values, the more smoothing will be applied and the longer it will take for the actor to reach the goal. Rotation Modes Two main rotation modes are supported: Look-at-Camera : The actor X axis is oriented towards the player head, with Z in the global \"up\" direction. Hand Rotation : The actor aligns with the palm. X axis is facing the inside of the palm, Z aligns with the direction of fingers. The zone direction and the rotation can be configured independently. For example the zone offset can be aligned with the palm, while the rotation faces the camera: Constraint Activation The constraint becomes active when a usable hand could be found, which matches the Hand property. If Any Hand is selected, either left or right hand will be used, depending on which hand starts tracking first. If the current Tracked Hand is lost the opposite hand will be used. The constraint becomes inactive when neither hand is found. By default the constraint will always have a valid goal if a usable hand is tracking. Extended constraint variants can have further conditions, such as the Palm-Up constraint which only becomes when the hand faces the camera. Events Constraint Activated/Deactivated : Called when the constraint becomes active or inactive respectively. The basic HandConstraintComponent only deactivates when hand tracking is lost. Extended hand constraint classes can have additional conditions. For example the Palm-Up constraint also requires that the palm is facing the camera. Begin/End Tracking : Called when a hand starts tracking or when tracking is lost. This includes the case where the Hand settings is 'Any Hand' and tracking switches from one hand to the other. In this case first the EndTracking event for the old hand is called and then the BeginTracking event for the new hand."
  },
  "Docs/HandInteraction.html": {
    "href": "Docs/HandInteraction.html",
    "title": "Hand Interaction | UXT Documentation",
    "keywords": "Hand interaction Hand interaction with UX elements is performed via the hand interaction actor . This actor takes care of creating and driving the pointers and visuals for near and far interactions. Near interactions are performed by either grabbing elements pinching them between index and thumb or poking at them with the finger tip. While in near interaction mode a finger cursor is displayed on the finger tip to provide feedback about the closest poke target. Far interacions are performed pointing via a ray attached to the hand with selection triggered by pressing index and thumb tips together. A far beam is displayed representing the ray shooting out of the hand. At the end of the beam a far cursor gives feedback about the current far target. Poke and grab targets are defined by adding a component implementing the grab target interface and poke target interface . All visible objects with collision will be hit by the far ray by default but only components implementing the far target interface will receive far interaction events. Provided UX elements like the pressable button implement these interfaces to use interactions to drive their state. Hand interaction actor Add a <xref:_a_uxt_hand_interaction_actor> to the world per hand in order to be able to interact with UX elements. There is no other additional setup required, just remember to set the actors to different hands via their Hand property as by default they use the left hand. See MRPawn in UXToolsGame for an example of hand interaction setup. The actor will automatically create the required components for near and far pointers and their visualization. Properties controlling the setup of these are exposed in the actor directly. A few ones deserving special attention are explained in the following sections. Near activation distance Each hand will transition automatically from far to near interaction mode when close enough to a near interaction target. The near activation distance defines how close the hand must be to the target for this to happen. Trace channel The hand actor and its pointers perform a series of world queries to determine the current interaction target. The trace channel property is used to filter the results of those queries. Default visuals Default visuals are created for near and far cursor and far beam in the form of the following components: Near cursor: <xref:_u_uxt_finger_cursor_component> Far cursor: <xref:_u_uxt_far_cursor_component> Far beam: <xref:_u_uxt_far_beam_component> In order to allow for custom visuals, their creation can be individually disabled via properties in the advanced section of the Hand Interaction category. See also Mixed Reality Instinctual Interactions : design principles behind the interaction model. <xref:_i_uxt_grab_target> <xref:_i_uxt_poke_target> <xref:_i_uxt_far_target>"
  },
  "Docs/InputSimulation.html": {
    "href": "Docs/InputSimulation.html",
    "title": "Input Simulation | UXT Documentation",
    "keywords": "Input Simulation Input simulation is using mouse and keyboard input in the editor to simulate a Head-Mounted Display (HMD) as well as hand tracking and other features. This allows testing Mixed-Reality features, such as buttons and interactions, without a physical device. Input simulation is not intended as a long term substitute for actual hardware, but as a development tool to improve iteration times and allow testing in case of limited hardware availability. Input simulation is only enabled in the Unreal editor. Activation Input simulation will become active when playing in the Unreal editor and no head-mounted display (HMD) is connected. If a HMD is connected the input simulation is disabled and camera placement is controlled by the device. Controls Default controls for moving the camera are based on the DefaultPawn: W/S keys for moving forward/backward A/D keys for moving sideways Q/E keys for moving down/up Mouse movement controls head rotation In addition to default pawn movement there are virtual hands that can be controlled for simulating hand tracking: Hold Left Shift/Alt for controlling the left/right hand respectively. This disables head rotation with the mouse and moves the hand instead. Both hands can be controlled together by holding both control keys. T/Y keys for toggling hand visibility. UX Tools project settings contain an option to \"Start with Hands Enabled\". Hands can still be enabled with the T/Y keys even if they are not initially visible. Press the Left mouse button to perform a \"Pinch\" gesture. This also activates the \"grasp\" state of the hand controller, allowing object manipulation. Press the Middle mouse button to perform a \"Poke\" gesture. This can be used for poking buttons. Note that buttons can also be pressed by simply moving the hand towards the button, without performing any gestures. Holding the Right mouse button enables rotation of the controlled hands: X axis changes yaw angle. Y axis changes pitch angle. Scroll wheel changes roll angle. To reset the hand rotation press the visibility keys (T/Y) twice. This will reset the hand location and rotation to defaults. Hand Animation Hand input is simulated with skeletal meshes that are animated according to user input. The main input simulation actor adds two instances of the skeletal hand mesh for the left and right hand respectively. The mesh asset is expected to model the right hand, while the left hand mesh is mirrored on the Y axis. An animation blueprint is used to toggle between different possible hand poses, such as a default relaxed hand, poking with the index finger, or pinching between thumb and index finger. Changing the current pose is bound to user input, usually the left and middle mouse buttons. The animation blueprint handles transition and blending between poses. After the mesh animation, the input simulation actor reads the position and rotation of bones matching the relevant hand joints by name (see EWMRHandKeypoint enum). This data is then passed to the input simulation subsystem to emulate device data when requested. Updating Hand Animation Assets The hand animation is best created from an FBX file. The file should contain: A mesh model for the right hand. A skeleton that animates the mesh. Bones in the skeleton that match the EWMRHandKeypoint enum by name: Palm, Wrist, ThumbMetacarpal, ThumbProximal, ThumbDistal, ThumbTip, IndexMetacarpal, etc.. Note that these bones do not necessarily have to deform the mesh, they only define the simulated joint positions. However, the joint bones should be aligned with the visible mesh to avoid confusion. Animation poses for the relevant hand gestures: Flat, Relaxed, Poke, Pinch, Grab, PalmUp, etc. The FBX file is imported, generating a mesh, skeleton, material(s), and animation assets. Open the hand animation asset that contains the desired hand poses. With Create Asset > Create PoseAsset > Current Animation create a pose asset that can blend between the various hand poses. Open the new pose asset and rename the relevant poses with meaningful names, e.g. \"Flat\", \"Relaxed\", \"Pinch\", \"Poke\". Open the InputSimulationHands_AnimInstance asset. This is the animation blueprint that drives the skeletal animation. In the AnimGraph find the PoseAsset blend node and in the Details panel change the linked pose asset to the one created above."
  },
  "Docs/Installation.html": {
    "href": "Docs/Installation.html",
    "title": "Installation Guide | UXT Documentation",
    "keywords": "Installation Guide Prerequisites Before getting started with UX Tools, make sure that you have installed the required tools . Getting the prebuilt plugin If you just want to add UXT to your game project, the quickest way is through the packaged plugin provided in the release page: Download the packaged plugin zip from the latest release page (e.g. UXTools.0.9.0.zip ). Unzip the file directly into your project's Plugins folder. The Plugins folder should be located at the root of your project, where the .uproject file is. Create it if it doesn't exist. Make sure your game project is a code one, as opposed to blueprint-only, if you are planning to package it for HoloLens. Otherwise UE will fail to package it because it can't build the plugin sources. Open your project and enable the UX Tools plugin in the plugins menu. You now have access to all of the plugin features. The first thing you want to do is probably add a hand interaction actor per hand to your map or pawn so you can use your hands to drive the controls and behaviors provided in UXT. Next steps HoloLens 2 tutorial series Unreal development journey"
  },
  "Docs/Manipulator.html": {
    "href": "Docs/Manipulator.html",
    "title": "Manipulator Component | UXT Documentation",
    "keywords": "Manipulator Components Manipulator components allow an actor to be picked up by a user and then moved, rotated or scaled. Generic Manipulator The Generic Manipulator component is a general-purpose implementation of the Manipulator Component Base . It supports both one and two-handed manipulation with a number of configurable settings to change its behavior. One-handed manipulation If one-handed manipulation is enabled the actor can be moved with just one hand. This mode supports movement and rotation, but not scaling of the actor. The way hand rotation translates into actor rotation depends on the One Hand Rotation Mode : Maintain Original Rotation : Does not rotate object as it is being moved. Rotate About Object Center : Only works for articulated hands/controllers. Rotate object using rotation of the hand/controller, but about the object center point. Useful for inspecting at a distance. Rotate About Grab Point : Only works for articulated hands/controllers. Rotate object as if it was being held by hand/controller. Useful for inspection. Maintain Rotation To User : Maintains the object's original rotation for Y/Z axis to the user. Gravity Aligned Maintain Rotation To User : Maintains object's original rotation to user, but makes the object vertical. Useful for bounding boxes. Face User : Ensures object always faces the user. Useful for slates/panels. Face Away From User : Ensures object always faces away from user. Useful for slates/panels that are configured backwards. Two-handed manipulation If two-handed manipulation is enabled the actor can be moved, rotated, and scaled by grabbing it with both hands. Each of these actions can be enabled or disabled separately as needed, e.g. an actor can have rotation and scaling enabled while movement is disabled. Movement uses the center point between both hands, so each hand contributes half of the translation. Rotation is based on imaginary axis between both hands. The actor will rotate with the change of this axis, while avoiding roll around it. Scaling uses the change in distance between hands. Smoothing The generic manipulator has a simple smoothing option to reduce jittering from noisy input. This becomes especially important with one-handed rotation, where hand tracking can be unreliable and the resulting transform amplifies jittering. The smoothing method is based on a low-pass filter that gets applied to the source transform location and rotation. The resulting actor transform T_final is a exponentially weighted average of the current transform T_current and the raw target transform T_target based on the time step: T_final = Lerp( T_current, T_target, Exp(-Smoothing * DeltaSeconds) ) Notes When using the Generic Manipulator with a Procedural Mesh , you will need to: Disable \"Use Complex as Simple Collision\" on the Procedural Mesh . Set \"Create Collision\" when creating the Procedural Mesh . This is due to UXTools only querying for simple collision volumes when detecting interaction targets, in order to ensure correct detection in all situations. You can read more about simple vs complex collisions here ."
  },
  "Docs/PalmUpConstraintComponent.html": {
    "href": "Docs/PalmUpConstraintComponent.html",
    "title": "Palm-Up Constraint Component | UXT Documentation",
    "keywords": "Palm-Up Constraint Component Hand constraint specialization that activates only when the palm is facing the player. Usage Create a PalmUpConstraintComponent on an Actor. See Hand constraint documentation for common settings. The Palm-Up constraint will activate when the palm normal is within a cone of size Max Palm Angle of the camera direction. Optionally the constraint can also use a hand flatness condition by enabling the Require Flat Hand option. Flatness is approximated by checking the triangle between palm, index finger tip and ring finger tip. If the triangle aligns to the palm within the Max Flat Hand Angle the hand is considered flat."
  },
  "Docs/PinchSlider.html": {
    "href": "Docs/PinchSlider.html",
    "title": "Pinch Slider | UXT Documentation",
    "keywords": "Pinch slider A pinch slider component allows the user to continuously change a value by moving the slider thumb along the track. Creating a pinch slider from scratch The first step of creating a slider from scratch is adding the UxtPinchSliderComponent to an actor blueprint. This is a low level component that drives slider logic. The pinch slider visuals are made of three primary components. Slider thumb : This component is required. It's the static mesh that the user interacts with Slider track : This component is optional. It's the static mesh is scaled to match the range of the slider travel Tick marks : This component is optional. It's an instanced static mesh that is used to represent ticks along the track of the slider Add a StaticMeshComponent to the actor and name it \"SliderThumb\" (alternatively, name it something else and and set the ThumbVisuals property of the UxtPinchSliderComponent to reference this new mesh). If you want to add a track to the slider, add a StaticMeshComponent to the actor and name it \"SliderTrack\" (alternatively, name it something else and and set the TrackVisuals property of the UxtPinchSliderComponent to reference this new mesh). If you want to add tick marks to the slider, add an InstancedStaticMesh to the actor, set a mesh for instancing, and name it \"TickMarks\" (alternatively, name it something else and and set the TickMarkVisuals property of the UxtPinchSliderComponent to reference this new instanced mesh). It is important to note that the UxtPinchSliderComponent uses the mesh assigned to the ThumbVisuals property to construct a BoxComponent that is used for grab and far interactions. The UxtPinchSliderComponent uses the mesh extents to create this box collider. If the slider is configured correctly, the slider should now be grabbable via near and far interaction and update its position and value based on user input. Here are the events that you can use to hook the slider value up to you application logic: OnValueUpdated : This event is called whenever user interaction causes the value of the slider to change (i.e. it is moved) OnInteractionStarted : This event is called when a user starts grabbing the slider thumb (either near or far interaction) OnInteractionEnded : This event is called when a user stops grabbing the slider thumb (either near or far interaction) OnFocusEnter : This event is called when a pointer starts giving focus to the slider (either near or far interaction) OnFocusExit : This event is called when a pointer starts giving focus to the slider (either near or far interaction) OnStateUpdated : This event will be called whenever the slider changes its internal state. It supplies a EUxtSliderState value that represents the new state (Default, Focus, Grab). Here is an example of the OnValueUpdated event being used: Instance Editable Properties SliderValue The current value of the slider in 0-1 range SliderStartDistance Where the slider track starts, as distance from center along slider axis, in local space units SliderEndDistance Where the slider track ends, as distance from center along slider axis, in local space units. NumTickMarks Number of tick marks to add to the slider TickMarkScale Scale of the tick marks on the slider ThumbVisuals Visual representation of the slider thumb TrackVisuals Visual representation of the track TickMarkVisuals Visual representation of the tick marks CollisionProfile Collision profile used by the slider thumb"
  },
  "Docs/PressableButton.html": {
    "href": "Docs/PressableButton.html",
    "title": "Pressable Button | UXT Documentation",
    "keywords": "Pressable button A button gives the user a way to trigger an immediate action. It is one of the most foundational components in mixed reality. Creating a pressable button from scratch The first step of creating a button from scratch is adding the UxtPressableButtonComponent to an actor blueprint. This is a low level component that drives button logic. This is followed by creating some moving visuals. Moving visuals are essential for pressable button interaction as they're visualizing pressed and released state. Add a StaticMeshComponent (or any SceneComponent ) to the actor and set the visuals property of the UxtPressableButtonComponent to reference this new mesh. Also ensure that the local positive x-axis of this mesh component points in the direction the button is expected to be pushed. Any component children of this mesh will move along with it as the button is pushed. It is important to note that the UxtPressableButtonComponent uses the component assigned to the visuals property to construct a BoxComponent that is used for poke and far interactions. The UxtPressableButtonComponent uses the the visuals property component bounds, and child bounds, to create this box collider. If the button is configured correctly, the button should now react to presses during play. As well as this, the button planes visualizations will be visible in editor while UxtPressableButtonComponent is selected. These planes represent some of the properties of the button. The solid white plane is the front plane. if the finger crosses this plane from the front, the button will be pushed . The front plane aligns with the front face of the BoxComponent discussed earlier The dashed light grey plane is drawn so that it is PressedDistance away from the front plane. If the finger crosses this plane from the front, the button will be pressed . There is more information about this under Pressed Fraction . The solid dark grey plane is drawn so that it is MaxPushDistance away from the front plane. The moving visuals will not move beyond this plane. There is more information about this under Max Push Distance . A distinction is being made here between push and press . A button can be pushed without being pressed. A button will only fire OnButtonPressed once it has been pushed beyond the PressedDistance. If a button needs to respond to push, OnBeginPoke and OnEndPoke can be used. Some buttons may also have static visuals. Static visuals can be created by adding another mesh component, making sure that it is not a child of the moving visuals. Configuring the component hierarchy so that the buttons work should be simple. The only things that are essential are that the pressable button component is not a child of the moving visuals. Also ensure that only visuals that should move with press are children of the moving visuals. Here are some examples of configurations of the button hierarchy that will work: Although this button is behaving correctly, it's not doing anything useful. The pressable button events can be used in a blueprint in order to respond to press/release. There are a few useful events that can be used: OnBeginFocus : This event is called when a pointer starts giving focus to the button. OnUpdateFocus : This event is called for every tick that the button has focus from a pointer. A button will hold focus while it is being pressed. OnEndFocus : This event is called when a pointer stops giving focus to the button. OnBeginPoke : This event is called when a near pointer starts pushing the button. See description above for the difference between a push and a press. OnUpdatePoke : This event is called tick that a pointer is pushing the button. See description above for the difference between a push and a press. OnEndPoke : This event is called when a near pointer stops pushing the button. See description above for the difference between a push and a press. OnButtonPressed : Called when the current push distance of the button exceeds the pressed fraction . OnButtonReleased : Called when the current push distance of the button subceeds the released fraction . OnButtonEnabled : Called when the button is enabled. OnButtonDisabled : Called when the button is disabled. This will not raise a release event if the button is disabled while pressed. Here are some examples of these events in use in the SimpleButton blueprint sample provided with UXT: HoloLens 2 Button Blueprint The HoloLens 2 button blueprint, named BP_ButtonHoloLens2 , is a button blueprint that provides configurable HoloLens 2 Shell style visuals and behaviors. Please see the graphics documentation for more information about the button's shaders and materials. Visual Configuration To aid in the time it takes to configure buttons, a handful of blueprint variables are exposed which react to changes made during edit time and runtime. For example, changing the Button Size from (16, 32, 32) to (16, 64, 32) will automatically scale the button's front and back plates to create a wide button without effecting the button icon or label. Updating the Button Label will automatically adjust the button's text render component. Changing the Button Icon to a new unicode code point will generate the appropriate unicode character to index into a font containing the icon (e.g. Font_SegoeMDL2_Regular_42 ). Note, the font atlas will need to be updated to support any new icons which are not already present within the font atlas. To add a new icon, open the icon font, such as Font_SegoeMDL2_Regular_42 . Under \"Import Options\" select the \"Chars\" property. Paste your icon's unicode character into the \"Chars\" property and save the font. Finally reimport the font uasset. Scripting Logic All UxtPressableButtonComponent events are passed up to the parent BP_ButtonHoloLens2 via the BP_BaseButton base class. So, any BP_BaseButton variables in other blueprints can easily bind to button events from the variable details panel without having to search for a child button component. In the below example \"Hello\" is printed when a BP_BaseButton , or any derived classes, are pressed: HoloLens 2 Button Variants A handful of derived BP_ButtonHoloLens2 blueprints exist to exhibit behavior not found on a typical pressable button. These variants include: BP_ButtonHoloLens2Toggle , displays an additional back plate based on the button's toggle state. BP_ButtonHoloLens2ToggleCheck , displays a check box icon based on the button's toggle state. BP_ButtonHoloLens2ToggleSwitch , displays a switch icon based on the button's toggle state. Public Properties Push Behavior How the visuals should react when the button is pressed. Translate means the visuals move move along the local x-axis. Compress means the visuals will scale along the x-axis. Note, when compressed the visual's pivot should align with the back face of the compressible region. In other words, the plane visualized by the max push distance . Max Push Distance The maximum distance the button can move. Pressed Fraction The fraction of the maximum push distance that an unpressed button needs to be pushed in order for the button pressed event to fire. Released Fraction The fraction of the maximum push distance that a pressed button needs to be pushed in order for the button released event to fire. Recovery Speed The speed at which the button visuals return to the their resting position when no longer being touched by near interaction or selected using far interaction. Front Face Collision Margin The distance in front of the visuals front face to place the front of the button box collider. Visuals A reference to the scene component that represents the moving part of the button. The extents of the button collider will also be constructed using this scene component and child bounds. Collision Profile The collision profile used for the button collider, which is constructed using the moving visuals mesh component extents."
  },
  "Docs/ReleaseNotes.html": {
    "href": "Docs/ReleaseNotes.html",
    "title": "Release Notes | UXT Documentation",
    "keywords": "UX Tools 0.9.0 release notes What's new Breaking changes Known issues This release of the UX Tools supports only HoloLens 2. Support for other MR platforms remains a goal for us but is not the current focus. Unreal 4.25 required. What's new HoloLens 2 style buttons Expanding on our previous pressable button work, the toolkit now provides blueprints that mimic HoloLens 2 button visuals and behaviors. The goal of the BP_ButtonHoloLens2 blueprint is to be easily configurable via the details panel. For examples please see the ButtonExample level. A handful of derived BP_ButtonHoloLens2 blueprints exist to exhibit behavior not found on a typical pressable button. These variants include: BP_ButtonHoloLens2Toggle , which displays an additional back plate based on the button's toggle state. BP_ButtonHoloLens2ToggleCheck , which displays a check box icon based on the button's toggle state. BP_ButtonHoloLens2ToggleSwitch , which displays a switch icon based on the button's toggle state. Pinch slider We're increasing our collection of UX controls with another classic: the slider . We now provide a low level component to construct your own sliders with support for both far and near interaction. There's also a simple, customizable slider blueprint for out-of-the-box use. Improved far beam visuals The visuals for the far beam displayed when in far interaction mode now match closely the ones in the HoloLens 2 Shell. The beam is dashed and fades out away from the end points to be less intrusive by default and becomes solid and bends while dragging to give a better sense of physical connection. Generic manipulator improvements Custom manipulation targets The UUxtGenericManipulatorComponent now allows you to target any SceneComponent as the subject for the manipulation. You can find this in the advanced properties for the Generic Manipulator. Interaction mode toggle You can now find an Interaction Mode setting in the UUxtGenericManipulatorComponent that allows you to toggle far or near interaction with the object. Constraint system UUxtGenericManipulatorComponent now comes with a constraint system that allows for more flexible usage of translation, rotation and scale constraints. In this version we've added UUxtMoveAxisConstraint for constraining movement along a certain axis while interacting with an object. There's going to be a variety of other constraints in future releases. Manipulation flags modifiable in Blueprints Flags used by UUxtGenericManipulatorComponent , like Manipulation or Interaction Mode can now be used and set as Bitflags in Blueprints. Performant shaders and material functions New shaders and rendering techniques were added to implement Fluent Design System principles, and remain performant on Mixed Reality devices. Please see the graphics documentation for more information. New editor utility Blueprints and widgets Editor tools were created to make it easier to configure and align UX controls. For more details see the utilities documentation . Configurable front face fraction for buttons Similar to the existing pressed and released fractions on the pressable button, the new FrontFaceCollisionFraction can be used to move the pokable button volume some fraction of the max push distance in front of the visuals front face. Hand constraints Actors can now be made to follow hands by adding this constraint component. The actor will be placed next to the hand without overlapping it. See documentation for details. The Palm-Up constraint is an extended constraint which becomes enabled when the hand is facing the user. This is particularly useful for hand menus. See documentation for details. New touchable volume component Component that emits focus and poke events like the Pressable Button, but uses arbitrary touchable volumes. A primitive component must be added to the same actor. Events can be used in blueprints to react to user interaction. Forced grab cancelation Grab target interactions can now be force-cancelled by calling the ForceEndGrab function. Breaking changes PushBehaviour and MaxPushDistance made private in pressable button component UUxtPressableButtonComponent::PushBehavior and UUxtPressableButtonComponent::MaxPushDistance have been moved to private to enforce MaxPushDistance constraints with compressible buttons (MaxPushDistance is auto calculated for buttons with a compress push behavior). Please use the associated getters/setters from now on. Button states UUxtPressableButtonComponent 's IsFocused() , IsPressed() and IsDisabled() have been have been removed in favor of GetState() . These don't map 1:1, as before it was possible for IsFocused() and IsPressed() to be true simultaneously. GetState() will only give you the primary state of the button (e.g. Pressed). You can track the sub-states of the button using the begin/end events triggered by the button or combinations of overall states. Add option to switch between local and world space for button distances MaxPushDistance is now in button local space. There are two ways of fixing this if you wish the button planes to remain in the same positions in space as they were before updating uxt: If you want button distances in world space, record the value of Max Push Distance , switch the button property Use Absolute Push Distance to true and set Max Push Distance to the old value. If you want button distances in local space, complete step 1. and then switch the button property Use Absolute Push Distance back to false. The Max Push Distance will update automatically. Fix buttons so that they face positive X Buttons now depress towards negative x. This means that buttons will need to be rotated in order to function properly. Open your button blueprint, if you select the pressable button component you'll notice that the button planes are now aligned with the wrong side of the button. You should see a plane drawn with a solid white line on what you used to consider the back of the button visuals. Transform your movable button visuals so that the new front face is at the same location as the old front face, except reflected on the other side of the blueprint origin. Depending on your individual case, You may not need to transform your visuals at all, you may get away with only translating or you may need to both translate and rotate your button visuals. Here's a rough visualization of this reflection: Before Update After Update After Transform • ¦ | • | ¦ | ¦ • | → Front Plane, ¦ → Back Plane, • → Blueprint Origin Translate or rotate any other parts of your button blueprint so that it works with the new button orientation and location (e.g. rotate text to face new front plane, move baseplate so that it aligns with the new back of the button). Wherever this blueprint is used (maps, other blueprints etc.) you will need to rotate the actor 180° around the z-axis. Rename the UxtBoundingBoxManipulatorComponent and related classes to BoundsControl Existing assets referencing those types will have to be re-saved as the redirects for the names changes will be eventually removed. Code references to the renamed types will have to be updated. Shader compilation errors when ray tracing is supported If you are running UE 4.25.0 and your computer's GPU supports ray tracing you must disable UE 4.25 ray tracing support for UXT's custom shaders to compile. To disable ray tracing navigate to C:\\Program Files\\Epic Games\\UE_4.25\\Engine\\Config\\Windows (or where your installation is located), open DataDrivenPlatformInfo.ini and change bSupportsRayTracing=true to bSupportsRayTracing=false This setting change is not required in UE 4.25.1 (or later) or people with Epic's 13205426 changelist. This warning is also in the Getting Started portion of the README. Fix UxtFollowComponent to work with +X convention UUxtFollowComponent now works appropriately, following (Actor's) +X convention. That means that, if an asset was rotated by 180º in any subcomponent to be displayed as expected, that should now be reverted, rotating the Actor appropriately instead. Known issues Visual glitch with BoundsControl when the affordances overlap This can be worked around by preventing the actor from being scaled down past the point at which this occurs. Buttons can be pressed from behind If you stand behind a button you can press it by moving the finger through the button and back. Generic manipulator Rotation about object center via generic manipulator should use wrist rotation. Not working if physics is enabled and mesh is not actor root."
  },
  "Docs/Utilities.html": {
    "href": "Docs/Utilities.html",
    "title": "Utilities | UXT Documentation",
    "keywords": "Utilities UX Tools contains a handful of utilities that augment the Unreal Engine editor. Editor Utility Blueprints Editor utilities can be authored using scripted actions . Scripted actions are accessed by right-clicking actors or assets. Align Actors The Align Actors action aides in the layout of UX controls, or any actor type. To access the action select multiple actors you wish to align. Then right-click in a viewport or outliner window. Finally, select Scripted Actions > Align Actors . A properties window will pop up prompting for alignment settings. Note, the first actor selected is used as the alignment origin. The actor's bounds are used to ensure actors don't interpenetrate when aligned. Editor Utility Widgets Editor utility widgets can be used to modify the User Interface (UI) of the Unreal Editor. Icon Brush Editor The Icon Brush Editor editor utility widget aides in editing a UxtIconBrush by visually searching though the characters in a UFont . To open the editor click the \"Open Icon Brush Editor\" button from any UxtIconBrush details panel. Once opened, the editor should display a window similar to the one below: Selecting different icons, outlined in green, will apply the icon selection to the current UxtIconBrush . The Icon Brush Editor comes in handy when selecting new icons for controls. The BP_ButtonHoloLens2 blueprint makes the viewer accessible via the top level details panel :"
  },
  "Docs/WelcomeToUXTools.html": {
    "href": "Docs/WelcomeToUXTools.html",
    "title": "Welcome to UX Tools | UXT Documentation",
    "keywords": "Welcome to UX Tools UX Tools is the first MRTK-Unreal component to be released and is currently only supported on HoloLens 2. The component plugin includes code, blueprints, and example assets of common UX features for input simulation, hand interaction actors, press-able button components, manipulator components and follow behavior components. Development If you're new to MRTK or Mixed Reality development in Unreal, we recommend starting at the beginning of our Unreal development journey . The Unreal development journey is the recommended starting point for MRTK and UX Tools , specifically created to walk you through installation, core concepts, and usage. Caution The Unreal development journey currently uses UX Tools 0.9.x and Unreal 4.25.3 or later . If you're working with other configurations it's still recommended that you start there, but you can also refer to the installation instructions . Documentation Versioning We have complete documentation for all release versions, but we recommend using the highest numbered public release branch for stability. Versions can be selected from the dropdown at the top-right of the screen: Getting help If you run into issues caused by UX Tools or have questions about how to do something, please file an issue on the GitHub repo."
  },
  "README.html": {
    "href": "README.html",
    "title": "What are the UX Tools? | UXT Documentation",
    "keywords": "What are the UX Tools? UX Tools for Unreal Engine is a UE game plugin with code, blueprints and example assets created to help you add in features commonly needed when you're developing UX for mixed reality applications. The project is still in early development (it provides a small set of features and breaking changes are to be expected) but the current features are complete and robust enough to use in your own projects. Note Only HoloLens 2 development is supported at the moment. Getting started with UX Tools Getting Started Feature Guides API Reference Required software Windows SDK 18362+ Unreal 4.25.3 or later Visual Studio 2019 To build apps with MRTK-Unreal and UX Tools, you need the Windows 10 May 2019 Update SDK The Unreal Engine provides support for building mixed reality projects in Windows 10 Visual Studio is used for code editing UX building blocks Input Simulation Hand Interaction Pressable Button Mouse and keyboard input in the editor to simulate a Head-Mounted Display and hand tracking Hand interaction actor that takes care of creating and driving pointers and visuals for interactions A button gives the user a way to trigger an immediate action Pinch Slider Bounds Control Manipulators A component allows the user to continuously change a value by moving the slider thumb along the track A component that allows the user to change the position, rotation, and size of an actor A component that allows an actor to be picked up by a user and then moved, rotated or scaled Follow Behavior Hand Constraints Palm Constraints A component used to keep objects \"following\" the user by applying a set of constraints on the component's owner A component that calculates a goal based on hand tracking and moves the owning actor A hand constraint component specialization that activates only when the palm is facing the player Far Beam A component allowing the user to visualize elements in the scene they can interact with from afar Tools Utilities Graphics Plugin containing a handful of tools that augment the Unreal Engine editor Breakdown of shaders, materials, and graphics techniques used to render UX components Example maps If you want to explore the different UXT features or want a reference for how to use them we recommend having a look at the example maps contained in the UX Tools Game (/UXToolsGame) in this repository. For that you should: Clone this repository. Checkout public/0.9.x. Bear in mind that this branch is alive. It's not a release, and will be updated regularly with potentially breaking changes . There will be a release tag (e.g. release/0.9.0) marked as such in GitHub. You can now open the UX Tools Game (/UXToolsGame) and explore individual example maps or open the Loader level to access some of the examples from a centralized hub. Packaged UX Tools game We also provide the UX Tools game pre-packaged for HoloLens 2 so you can try out the main UXT features directly on device easily. To use it: Obtain the packaged game from the latest release page (e.g. UXTGame-HoloLens.0.9.0.zip ) and unzip it to a local directory. Install it in the device via the Device Portal . Documentation The latest version of the documentation can be found here . Feedback and contributions Due to the early stage of the project and the likelihood of internal refactors, we are not in a position to accept external contributions through pull requests at this time. However, contributions and feedback in the shape of bug reports, suggestions and feature requests are always welcome!"
  }
}